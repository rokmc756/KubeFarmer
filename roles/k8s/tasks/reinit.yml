# Reinit Kubernetes
- name: Unset Variables of Token and Certs Generated Previously by Kubeadm Init
  set_fact:
    k8s_token_ca_cert:
    k8s_token:


- name: Set SELinux Permissive Mode
  shell: |
    setenforce 0
    sed -i 's/^SELINUX=enforcing$/SELINUX=permissive/' /etc/selinux/config
  when: ansible_distribution == "RedHat" or ansible_distribution == "CentOS" or ansible_distribution == "Rocky"


- name: Restart Containerd to Apply Changes in config.toml
  systemd:
    name: containerd
    state: restarted
  register: containerd_restarted
  until: containerd_restarted is succeeded
  retries: 20
  delay: 10
  # when: ansible_distribution == "openSUSE Leap"


- name: Reset Kubernetes
  command: kubeadm reset -f
  when: inventory_hostname in groups['master']


- name: Reset Kubernetes for Workers
  command: kubeadm reset -f
  when: inventory_hostname in groups['workers']


#- name: Restart Containerd to Apply Changes in config.toml
#  systemd:
#    name: containerd
#    state: restarted
#  register: containerd_restarted
#  until: containerd_restarted is succeeded
#  retries: 20
#  delay: 10
#  # when: ansible_distribution == "openSUSE Leap"


- name: Config image pull
  command: kubeadm config images pull
  when: inventory_hostname in groups['master']


- name: Initialize Kubernetes Master to Create Cluster
  shell: |
    kubeadm init --apiserver-advertise-address={{ hostvars[groups['master'][0]]['ansible_'~netdev1]['ipv4']['address'] }} \
    --pod-network-cidr={{ k8s.cni.pod_network }} --cri-socket=unix:///run/containerd/containerd.sock --ignore-preflight-errors=all --v=5
  async: 360
  poll: 5
  register: kubeadm_init
  when: inventory_hostname in groups['master']
- debug: msg={{ kubeadm_init }}
  when: print_debug == true and inventory_hostname in groups['master']


- name: Set Fact For Token CA Certificate
  set_fact:
    k8s_token_ca_cert: "{{ (kubeadm_init.stdout_lines | select('search', 'discovery-token-ca-cert-hash') | list | string | split(' '))[1] }}"
  delegate_to: "{{ item }}"
  delegate_facts: True
  with_items: "{{ groups['all'] }}"
  when: inventory_hostname in groups['master']
- debug: msg={{ k8s_token_ca_cert }}
  when: print_debug == true and inventory_hostname in groups['master']


- name: Set Fact For Token
  set_fact:
    k8s_token: "{{ (kubeadm_init.stdout_lines | select('search', '--token') | list | string | split(' '))[4] }}"
  delegate_to: "{{ item }}"
  delegate_facts: True
  with_items: "{{ groups['all'] }}"
  when: inventory_hostname in groups['master']
- debug: msg={{ k8s_token }}
  when: print_debug == true and inventory_hostname in groups['master']


- name: Enable Your User to Start Using the Cluster.
  shell: |
    rm -rf $HOME/.kube
    rm -rf "{{ k8s.base_path }}"/calico.yaml
    mkdir -p $HOME/.kube
    rm -f $HOME/.kube/config
    cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
    chown $(id -u):$(id -g) $HOME/.kube/config
  register: start_using_cluster_enabled
  when: inventory_hostname in groups['master']
- debug: msg={{ start_using_cluster_enabled }}
  when: print_debug == true and inventory_hostname in groups['master']


- name: Configure Kubectl Config
  shell: |
    kubectl taint nodes {{ hostvars[groups['master'][0]].ansible_hostname }} node-role.kubernetes.io/control-plane:NoSchedule-
  register: kubectl_configured
  until: kubectl_configured is succeeded
  retries: 10
  delay: 10
  when: inventory_hostname in groups['master']
- debug: msg={{ kubectl_configured }}
  when: print_debug == true and inventory_hostname in groups['master']
  # for higher versions than 1.26
  # kubectl taint nodes  {{ hostvars[groups['master'][0]].ansible_hostname }} node-role.kubernetes.io/control-plane:NoSchedule-
  # for earlier versions than 1.21
  # kubectl taint nodes --all node-role.kubernetes.io/master-


- name: Confirm that the Kubectl Command is activated.
  command: kubectl get nodes
  register: kubectl_get_nodes
  when: inventory_hostname in groups['master']
- debug: msg={{ kubectl_get_nodes }}
  when: print_debug == true and inventory_hostname in groups['master']


- name: Create POD for Master as well
  lineinfile:
    path: "{{ k8s.base_path }}/.bashrc"
    line: "{{ item.content }}"
    insertafter: EOF
    create: true
  register: master_pod_created
  with_items:
    - { content: " if [ -f /etc/bash_completion ] && ! shopt -oq posix; then " }
    - { content: "   . /etc/bash_completion " }
    - { content: " fi " }
    - { content: " source <(kubectl completion bash) " }
  when: inventory_hostname in groups['master']
- debug: msg={{ master_pod_created }}
  when: print_debug == true and inventory_hostname in groups['master']


- name: Setup Calico CNI Networking without Operator
  import_tasks: apply-calico.yml
  when: ( k8s.cni.name == "calico" and k8s.cni.operator == "" ) and inventory_hostname in groups['master']


- name: Setup Kube Flannel CNI Networking without Operator
  import_tasks: apply-kube-flannel.yml
  when: ( k8s.cni.name == "kube-flannel" and k8s.cni.operator == "" ) and inventory_hostname in groups['master']


- name: Setup Calico CNI Networking with Tigera Operator
  import_tasks: apply-tigera-operator.yml
  when: ( k8s.cni.name == "tigera" and k8s.cni.operator == "tigera" ) and inventory_hostname in groups['master']


- name: Confirm that the Kubectl Command is Activated.
  command: kubectl get nodes
  register: kubectl_get_nodes
  when: inventory_hostname in groups['master']
- debug: msg={{ kubectl_get_nodes }}
  when: print_debug == true and inventory_hostname in groups['master']


# Initialize Workder Nodes in Kubernetes Cluster
- name: Reset Kubernetes on Cluster Nodes
  command:  kubeadm reset -f
  register: kube_cluster_reset
  when: inventory_hostname in groups['workers']
- debug: msg={{ kube_cluster_reset }}
  when: print_debug == true and inventory_hostname in groups['master']


- name: Join the Other Kubernetes Nodes to Cluster, the Command must be Run on the Worker Nodes Only
  shell: |
    kubeadm join "{{ hostvars[groups['master'][0]]['ansible_'~netdev1]['ipv4']['address'] }}:6443" \
    --token "{{ k8s_token }}" --discovery-token-ca-cert-hash "{{ k8s_token_ca_cert }}"
  register: store_it_copied
  ignore_errors: yes
  when: inventory_hostname in groups['workers']
- debug: msg={{ store_it_copied }}
  when: print_debug == true and inventory_hostname in groups['workers']


- name: Confirm that the kubectl command is activated
  command: kubectl get nodes
  register: kubectl_get_nodes
  when: inventory_hostname in groups['master']
- debug: msg={{ kubectl_get_nodes }}
  when: print_debug == true and inventory_hostname in groups['master']


- name: Restart Kubelet in Master Node
  systemd:
    name: kubelet
    state: restarted
    enabled: yes
  register: kubelet_restarted
- debug: msg={{ kubelet_restarted }}
  when: print_debug == true and inventory_hostname in groups['master']


- name: Validate if Kubernetes Workers are all Ready
  shell: kubectl get nodes
  register: check_k8s_workers_status
  until: check_k8s_workers_status.stdout.find("NotReady") == -1
  retries: 30
  when: inventory_hostname in groups['master']
- debug: msg={{ check_k8s_workers_status }}
  when: print_debug == true and inventory_hostname in groups['master']


- name: Validate if All Containers are Running
  shell: kubectl get pods --all-namespaces -o wide
  register: all_containers_running_checked
  until: all_containers_running_checked.stdout.find("ContainerCreating") == -1
  retries: 60
  when: inventory_hostname in groups['master']
- debug: msg={{ all_containers_running_checked }}
  when: print_debug == true and inventory_hostname in groups['master']


- name: Stop and Disable Firewalld
  systemd:
    name: firewalld
    state: stopped
    enabled: no
  register: firewalld_started
- debug: msg={{ firewalld_started }}
  when: print_debug == true


- name: Enable and Restart Docker
  systemd:
    name: docker
    state: restarted
    enabled: yes
  register: docker_restarted
  ignore_errors: true
  until: docker_restarted is succeeded


- name: Enable and Restart Containerd
  systemd:
    name: containerd
    state: restarted
    enabled: yes
  register: containerd_restarted
  ignore_errors: true
  until: containerd_restarted is succeeded


# Calico CNI
# https://stackoverflow.com/questions/62139375/calico-ips-confusion
# https://www.skyer9.pe.kr/wordpress/?p=7317

# https://github.com/projectcalico/calico/issues/5711
# This is to do with the settings on kube-proxy. If your pod-cidr isn't in the range that's configured on kube-proxy,
# kube-proxy will NAT pod-pod traffic between different nodes, which will break network policy (though connectivity will appear to work)
# Typically you have to re-create the cluster with the correct pod-cidr defined.
# Fundamentally:
# kube-proxy and calico pod-cidr settings must agree (calico pod-cidr must be wholly within the kube-proxy cluster-cidr)
# host-cidr, pod-cidr and service-cidrs need to be disjoint (i.e. separate and non-overlapping)
#
# https://www.centlinux.com/2022/11/install-kubernetes-master-node-rocky-linux.html


---
- name: Disable Security with Firewalld and SELinux Policy
  import_tasks: security/disable-security.yml


- name: Create Default Containerd Directory
  file:
    path: "/etc/containerd"
    state: directory
    owner: root
    group: root
    mode: 0777
  register: create_default_containerd_directory
- debug: msg={{ create_default_containerd_directory }}
  when: print_debug == true


- name: Config Default Containerd
  shell: |
    containerd config default > /etc/containerd/config.toml
  notify:
    - Stop Docker
    - Stop Containerd
    - Stop Kubelet
    - Restart Containerd


- name: Force to Stop and Restart Services
  meta: flush_handlers


- name: Load Kernel Modules for Kurbernetes
  shell: |
    modprobe {{ item }}
  with_items:
    - overlay
    - br_netfilter


- name: Reset Kubernetes
  command: kubeadm reset -f
  when: inventory_hostname in groups['master'] or inventory_hostname in groups['workers']


- name: Remove Directories Used by Persistenct Volumes
  import_tasks: remove-pv-dirs.yml
  when: inventory_hostname in groups['master'] or inventory_hostname in groups['workers']


- name: Config Image Pull
  shell: |
    kubeadm config images pull --v=5
  register: config_image_pull
  until: config_image_pull is succeeded
  retries: 30
  delay: 15
  when: inventory_hostname in groups['master']


# kubeadm init --control-plane-endpoint={{ hostvars[groups['master'][0]][_netdev]['ipv4']['address'] }} --upload-certs \
- name: Initialize Kubernetes Master
  shell: |
    kubeadm init --apiserver-advertise-address={{ hostvars[groups['master'][0]][_netdev]['ipv4']['address'] }} --pod-network-cidr={{ k8s.cni.pod_network }} \
    --cri-socket=unix:///run/containerd/containerd.sock --ignore-preflight-errors=all --v=5 --upload-certs
  async: 360
  poll: 5
  register: kubeadm_init
  when: inventory_hostname in groups['master']
- debug: msg={{ kubeadm_init }}
  when: print_debug == true and inventory_hostname in groups['master']


- name: Set Fact For Token CA Certificate
  set_fact:
    k8s_token_ca_cert: "{{ (kubeadm_init.stdout_lines | select('search', 'discovery-token-ca-cert-hash') | list | string | split(' '))[1] }}"
  delegate_to: "{{ item }}"
  delegate_facts: True
  with_items: "{{ groups['all'] }}"
  when: inventory_hostname in groups['master']
- debug: msg={{ k8s_token_ca_cert }}
  when: print_debug == true and inventory_hostname in groups['master']


- name: Set Fact For Token
  set_fact:
    k8s_token: "{{ (kubeadm_init.stdout_lines | select('search', '--token') | list | string | split(' '))[4] }}"
  delegate_to: "{{ item }}"
  delegate_facts: True
  with_items: "{{ groups['all'] }}"
  when: inventory_hostname in groups['master']
- debug: msg={{ k8s_token }}
  when: print_debug == true and inventory_hostname in groups['master']


- name: Enable User as Admin Kube Environment
  import_tasks: enable-default-kube-env.yml
  when: inventory_hostname in groups['master']


- name: Configure Kubectl Config
  shell: |
    kubectl taint nodes {{ hostvars[groups['master'][0]].ansible_hostname }} node-role.kubernetes.io/control-plane:NoSchedule-
  register: kubectl_configured
  until: kubectl_configured is succeeded
  retries: 10
  delay: 10
  when: inventory_hostname in groups['master']
- debug: msg={{ kubectl_configured }}
  when: print_debug == true and inventory_hostname in groups['master']
  # For higher than 1.26.x version in above case
  # For earlier than 1.21.x version - kubectl taint nodes --all node-role.kubernetes.io/master-


- name: Create POD for Master
  lineinfile:
    path: "{{ k8s.base_path }}/.bashrc"
    line: "{{ item.content }}"
    insertafter: EOF
    create: true
  register: master_pod_created
  with_items:
    - { content: " if [ -f /etc/bash_completion ] && ! shopt -oq posix; then " }
    - { content: "   . /etc/bash_completion " }
    - { content: " fi " }
    - { content: " source <(kubectl completion bash) " }
  when: inventory_hostname in groups['master']
- debug: msg={{ master_pod_created }}
  when: print_debug == true and inventory_hostname in groups['master']


- name: Setup CNI Networking Plugin
  import_tasks: cni/setup-cni-plugin.yml
  when: k8s.cni.install == true and inventory_hostname in groups['master']


- name: Reset Kubernetes on Cluster Nodes
  shell: |
    kubeadm reset -f
  register: kube_cluster_reset
  when: inventory_hostname in groups['workers']
- debug: msg={{ kube_cluster_reset }}
  when: print_debug == true and inventory_hostname in groups['workers']


- name: Join the Other Kubernetes Nodes to Cluster, the Command must be Run on the Worker Nodes Only
  shell: |
    kubeadm join "{{ hostvars[groups['master'][0]][_netdev]['ipv4']['address'] }}:6443" --token "{{ k8s_token }}" --discovery-token-ca-cert-hash "{{ k8s_token_ca_cert }}"
  register: store_it_copied
  ignore_errors: yes
  when: inventory_hostname in groups['workers']
- debug: msg={{ store_it_copied }}
  when: print_debug == true and inventory_hostname in groups['workers']


- name: Validate if Worker Nodes are All Ready
  shell: |
    kubectl get nodes | sed 1d | awk '{print $2}' | grep NotReady | wc -l
  register: check_k8s_workers_status
  until: check_k8s_workers_status.stdout|int == 0
  retries: 30
  when: inventory_hostname in groups['master']
- debug: msg={{ check_k8s_workers_status }}
  when: print_debug == true and inventory_hostname in groups['master']


- name: Validate if All Containers are Running Normally
  shell: |
    kubectl get pods --all-namespaces -o wide | sed 1d | awk '{print $4}' | grep ContainerCreating | wc -l
  register: check_all_containers_running
  until: check_all_containers_running.stdout|int == 0
  retries: 60
  when: inventory_hostname in groups['master']
- debug: msg={{ check_all_containers_running }}
  when: print_debug == true and inventory_hostname in groups['master']


- name: Check the Status fo CNI Network Plugin
  import_tasks: cni/check-cni-network-plugin.yml
  when: k8s.cni.install == true and inventory_hostname in groups['master']


- name: Apply BGP Network for Calico
  import_tasks: cni/apply-bgp-network.yaml
  when: k8s.cni.calico.install == true and k8s.cni.calico.ipip.enable == true and inventory_hostname in groups['master']


- name: Apply Calico Network and Containerd
  import_tasks: cni/apply-cni-mtu.yaml
  when: k8s.cni.change_mtu == true


- name: Restart Services for Kubernetes After Reinitialization
  shell: |
    echo "Restart Services After Reinitialization"
  notify:
    - Stop Firewalld
    - Restart Docker
    - Restart Containerd
    - Restart Kubelet


- name: Force to Stop and Restart Services
  meta: flush_handlers


- name: Enable Helm Repository
  import_tasks: ext/enable-helm-repo.yml
  when: _helm.enable_repo == true and inventory_hostname in groups['master']


- name: Install Metallb
  import_tasks: ext/install-metallb.yml
  when: _metallb.install == true and inventory_hostname in groups['master']


- name: Cleanup the NVME Devices
  import_tasks: cleanup-nvme-devices.yml
  when: k8s.disk.nvme == true


# Calico CNI
# https://stackoverflow.com/questions/62139375/calico-ips-confusion
# https://www.skyer9.pe.kr/wordpress/?p=7317

# https://github.com/projectcalico/calico/issues/5711
# This is to do with the settings on kube-proxy. If your pod-cidr isn't in the range that's configured on kube-proxy,
# kube-proxy will NAT pod-pod traffic between different nodes, which will break network policy (though connectivity will appear to work)
# Typically you have to re-create the cluster with the correct pod-cidr defined.
# Fundamentally:
# kube-proxy and calico pod-cidr settings must agree (calico pod-cidr must be wholly within the kube-proxy cluster-cidr)
# host-cidr, pod-cidr and service-cidrs need to be disjoint (i.e. separate and non-overlapping)
#
# https://www.centlinux.com/2022/11/install-kubernetes-master-node-rocky-linux.html


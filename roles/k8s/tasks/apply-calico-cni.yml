# url: https://raw.githubusercontent.com/projectcalico/calico/v3.25.0/manifests/calico.yaml
- name: Download Calico Networking
  get_url:
    url: "{{ k8s.cni.calico.download_url }}/v{{ k8s.cni.calico.major_version }}.{{ k8s.cni.calico.minor_version }}.{{ k8s.cni.calico.patch_version }}/manifests/calico.yaml"
    dest: "{{ k8s.base_path }}/calico.yaml"
    mode: 0644
  register: calico_downloaded
  when: inventory_hostname in groups['master']
- debug: msg={{ calico_downloaded }}
  when: print_debug == true and inventory_hostname in groups['master']


- name: Modify {{ k8s.base_path }}/calica.yaml for CIDR
  lineinfile:
    dest: "{{ k8s.base_path }}/calico.yaml"
    regexp: "{{ item.regexp }}"
    line: "{{ item.line }}"
    state: "{{ item.state }}"
    backrefs: "{{ item.backrefs }}"
  register: modify_calica_yaml
  with_items:
    - { regexp: "^            # - name: CALICO_IPV4POOL_CIDR",                                line: "            - name: CALICO_IPV4POOL_CIDR",                                                      state: present, backrefs: yes }
    - { regexp: "^            #   value: \"192.168.0.0/16\"",                                 line: "              value: \"{{ k8s.cni.pod_network }}\"",                                            state: present, backrefs: yes }
    - { regexp: "^            # Enable IPIP",                                                 line: "            - name: IP_AUTODETECTION_METHOD\n              value: \"interface={{ netdev1 }}\"", state: present, backrefs: yes }
  when: inventory_hostname in groups['master']
- debug: msg={{ modify_calica_yaml }}
  when: print_debug == true and inventory_hostname in groups['master']


  # kubectl set env daemonset/calico-node -n kube-system IP_AUTODETECTION_METHOD=can-reach=<up of master node>
  # or
  # kubectl set env daemonset/calico-node -n kube-system IP_AUTODETECTION_METHOD=interface=eth.*


- name: Apply the MTU for BGP
  replace:
    path: "{{ k8s.base_path }}/calico.yaml"
    regexp: "{{ item.regexp }}"
    replace: "{{ item.line }}"
    backup: "{{ item.backup }}"
  register: apply_bgp_mtu
  with_items:
    - { regexp: "^  veth_mtu: \"0\"",                                                         line: "  veth_mtu: \"{{ cni_mtu }}\"",                                                  state: present, backup: no }
  when: k8s.cni.mtu == true and inventory_hostname in groups['master']
- debug: msg={{ apply_bgp_mtu }}
  when: print_debug == true and k8s.cni.mtu == true and inventory_hostname in groups['master']


- name: Replace {{ k8s.base_path }}/calica.yaml for CIDR
  replace:
    path: "{{ k8s.base_path }}/calico.yaml"
    regexp: "{{ item.regexp }}"
    replace: "{{ item.line }}"
    backup: "{{ item.backup }}"
  register: replace_calica_yaml
  with_items:
    - { regexp: "^  calico_backend: \"bird\"",                                                line: "  calico_backend: \"vxlan\"",                                                                   state: present, backup: no }
    - { regexp: "^            - name: CALICO_IPV4POOL_IPIP\n              value: \"Always\"", line: "            - name: CALICO_IPV4POOL_IPIP\n              value: \"Never\"",                      state: present, backup: no }
    - { regexp: "^            - name: CALICO_IPV4POOL_VXLAN\n              value: \"Never\"", line: "            - name: CALICO_IPV4POOL_VXLAN\n              value: \"Always\"",                    state: present, backup: no }
    - { regexp: "^            - name: CALICO_IPV6POOL_VXLAN\n              value: \"Never\"", line: "            - name: CALICO_IPV6POOL_VXLAN\n              value: \"Always\"",                    state: present, backup: no }
  when: k8s.cni.calico.vxlan.enable == true and inventory_hostname in groups['master']
- debug: msg={{ replace_calica_yaml }}
  when: print_debug == true and k8s.cni.calico.vxlan.enable == true and inventory_hostname in groups['master']
  #  - { regexp: "^  veth_mtu: \"0\"",                                                         line: "  veth_mtu: \"{{ k8s.cni.calico.vxlan.mtu }}\"",                                                state: present, backup: no }


- name: Delete Bird Lines In {{ k8s.base_path }}/calica.yaml
  lineinfile:
    dest: "{{ k8s.base_path }}/calico.yaml"
    regexp: "{{ item.regexp }}"
    state: "{{ item.state }}"
    backrefs: "{{ item.backrefs }}"
  register: delete_bird_lines
  with_items:
    - { regexp: "^              - -bird-live", state: absent, backrefs: yes }
    - { regexp: "^              - -bird-ready", state: absent, backrefs: yes }
  when: k8s.cni.calico.vxlan.enable == true and inventory_hostname in groups['master']
- debug: msg={{ delete_bird_lines }}
  when: print_debug == true and k8s.cni.calico.vxlan.enable == true and inventory_hostname in groups['master']


- name: Setup Calico Networking
  command: kubectl apply -f {{ k8s.base_path }}/calico.yaml
  register: calico_network_setup
  when: inventory_hostname in groups['master']
- debug: msg={{ calico_network_setup }}
  when: print_debug == true and inventory_hostname in groups['master']


- name: Check if Calicoctl Exists
  stat: path=/usr/local/bin/calicoctl
  register: calicoctl_bin_exists
- debug: msg={{ calicoctl_bin_exists }}
  when: print_debug == true


- name: Unarchive Calicoctl Tarball
  unarchive:
    src: "calicoctl-v{{ k8s.cni.calico.major_version }}.{{ k8s.cni.calico.minor_version }}.{{ k8s.cni.calico.patch_version }}.tar.gz"
    dest: "/usr/local/bin/"
    mode: 0755
    owner: root
    group: root
  register: unarchive_calicoctl_tarball
  when: calicoctl_bin_exists.stat.exists == false and upload_calico == true
- debug: msg={{ unarchive_calicoctl_tarball }}
  when: print_debug == true and calicoctl_bin_exists.stat.exists == false and upload_calico == true


# curl -o /root/calicoctl -O -L https://github.com/projectcalico/calicoctl/releases/download/v{{ k8s.cni.calicoctl.major_version }}.{{ k8s.cni.calicoctl.minor_version }}.{{ k8s.cni.calicoctl.patch_version }}/calicoctl
# curl -L https://github.com/projectcalico/calico/releases/download/v3.27.3/calicoctl-linux-amd64 -o /root/calicoctl
- name: Install Calicoctl
  shell: |
    curl -o {{ k8s.base_path }}/calicoctl -O -L https://github.com/projectcalico/calico/releases/download/v{{ k8s.cni.calico.major_version }}.{{ k8s.cni.calico.minor_version }}.{{ k8s.cni.calico.patch_version }}/calicoctl-linux-amd64
    chmod +x {{ k8s.base_path }}/calicoctl
    mv {{ k8s.base_path }}/calicoctl /usr/local/bin/
  register: install_calicoctl
  when: upload_calico == false and calicoctl_bin_exists.stat.exists == true and inventory_hostname in groups['master']
- debug: msg={{ install_calicoctl }}
  when: print_debug == true and upload_calico == false and calicoctl_bin_exists.stat.exists == false and inventory_hostname in groups['master']

#
# calicoctl ipam show
# calicoctl ipam show --show-blocks
# kubectl get nodes -o jsonpath='{.items[*].spec.podCIDR}' ;echo
# 172.16.0.0/24 172.16.1.0/24 172.16.2.0/24 172.16.3.0/24

# kubectl get node k8s-m -o json | jq '.spec.podCIDR'
# "172.16.0.0/24"

# calicoctl get ippool -o wide
# iptables -t filter -S | grep cali
# iptables -t nat -S | grep cali

# calicoctl get ippool -o wide


# calicoctl node run --ip=autodetect --ip-autodetection-method=can-reach=8.8.8.8
# calicoctl node run --ip=autodetect --ip-autodetection-method=can-reach=k8s_master_interface
# calicoctl node run --ip=autodetect --ip-autodetection-method=can-reach=k8s_node_production_interface


# https://isn-t.tistory.com/41

# Calico with kubernetes
# https://medium.com/@jeetanshu/networking-in-multi-node-kubernetes-cluster-the-role-of-calico-dcd78367845e

# Disable BGP and Enable VXLAN
# https://cwal.tistory.com/12


# https://github.com/rook/rook/issues/14445
# Pod Network
# Root cause of the problem:
#
# The pod CIDR in the clusters (192.168.0.0/16) was overlapping with the host network nodes IP addresses (192.168.35.x).
# Pod CIDR was configured when creating the cluster, and when deploying Calico.
#
# When Calico sees a 192.x.x.x IP, it considers it as an internal IP address to the cluster, so it will not do SNAT for these IPs. The problem was that my external server IP was considered as an internal IP because of the overlap. This was clearly a misconfiguration.
#
# How to fix this:
# If someone is in the same situation, he can check the current pod CIDR with kubectl get ippool -o yaml and compare it with the local network IPs.
# In order to solve this, I recreated the clusters with kubeadm and using the parameter --pod-network-cidr=10.200.0.0/16. Also, when deploying Calico, I replaced the default CIDR 192.168.0.0/16 with 10.200.0.0/16 (Calico custom resources).
# And if someone follows the "steps to reproduce" in the first post, you need to enable host networking with sed -i "s/#provider: host/provider: host/g" cluster.yaml in the Storage cluster, before applying the cluster.yaml file. This is required, so that the Ceph services can be properly accessed from outside the Kubernetes Storage cluster.
#
# Now I see that the pvc and pv are in the Bound status, so it works as intended. Feel free to close the issue.

# https://github.com/rook/rook/issues/5065
# https://github.com/projectcalico/calico/issues/1709#issuecomment-399350737
# https://docs.tigera.io/calico/latest/reference/architecture/data-path


# [ VXLAN Issue ]
# https://github.com/projectcalico/calico/issues/6160
#


